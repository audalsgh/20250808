# 35일차 - CUDA환경에서 GPU로 실시간 데이터 처리

## Numba란?
1. 정의
- 파이썬 함수를 고성능 기계어 코드로 변환하는 JIT(Just-In- Time) 컴파일러. 
- 등장배경 : 자율주행차는 초당 1920*1080 (FHD) 200만개의 픽셀 이미지 + 수백개의 거리, 속도 반환값 변환을 처리해야함
- 이 모든 데이터를 30FPS 기준 "33ms 이내"에 처리하여 인지, 판단, 제어를 완료해야함
- 직렬처리를 하는 CPU는 혼자서 200만개의 문제를 풀어야하고, 병렬처리를 하는 GPU는 수천명이서 픽셀 몇개씩을 나눠 맡아 동시에 처리하여 빠르다!

2. 작동원리
- 함수가 처음 호출될때, 입력되는 데이터 타입에 맞춰 "최적화된 코드"를 즉석에서 컴파일하는 것.
- CPU 메모리의 Numpy 배열을 GPU 메모리로 복사 -> 커널이 GPU에서 함수를 병렬로 실행시킴 (매우빠름) -> 다시 CPU 메모리로 결과 배열을 복사함.<br>(Host->Device->Host 순)
- 병목 : GPU의 연산이 아무리 빨라도 데이터 전송에 드는 시간이 존재하므로, 간단한 코드는 CPU에서만 하는게 좋다.
  
  <img width="565" height="682" alt="image" src="https://github.com/user-attachments/assets/886564f2-2a17-474e-a936-4d6cd3bf2ec5" /><br>
**-> 작은 데이터에선 GPU 가속을 해도 "데이터 전송시간" 비중이 더 커서 속도향상 체감이 제한적. (고작 0.4초 빠름)**<br>
**-> 큰 데이터에선 "연산시간"이 압도적으로 줄어들어, 동일한 "데이터 전송시간"을 가져도 걸린시간 총합이 적기에 빠르다고 느낀다. (0.7초 빠름!)**

3. 함정 : 치명적 상쇄(Catastrophic Cancellation)
- float32 = 약 7자리의 10진수 단정밀도
- float64 = 약 15~17자리의 배정밀도
- GPU는 보통 크기가 작은 float32 연산이 훨씬 빠르므로, 모든 연산이 float32로 일어나도록 데코레이터에 명시해준다.<br>
`@vectorize(['float32(float32)'], target='cuda')`<br>
- 정밀한 제어를 위해서라면 성능을 약간 희생하여 float64를 사용한다.<br>

  <img width="259" height="54" alt="image" src="https://github.com/user-attachments/assets/439c4d2d-afaa-48e0-a692-5492ad5a9341" /><br>
**-> float32연산은 1보다 매우작은 값을 무시하므로, 1e-8같은 작은 값을 반환하면 0이라고 나온다.**

## CUDA의 실행 모델 계층 구조 (Grid, Block, Thread)
GPU에 직접 명령을 내리는 커널
- Ufunc 자체는 간단하지만, 모든 연산이 배열의 각 원소에 독립적이여야만 사용가능.
- 이웃 픽셀 정보가 필요한 컨볼루션 등의 연산에는, 우리가 직접 스레드의 동작을 정의하는 "커스텀 CUDA 커널"을 작성해야함.
- nsys : 엔비디아의 프로파일러로, 코드의 어느 부분에서 시간이 소요되는지 정확히 알려줌.<br>
`nsys profile python my_script.py`
- 각 CUDA 커널의 평균 실행 시간, 메모리 복사에 소요된 시간, 메모리 처리량 [GB/s] 단위로 Coalescing이 잘 되었는지 판단하는 지표를 보여줌.

함정2. : 메모리 병합(Coalescing)
```
  ┌───────────┬───────────┬───────────┬───────────┬───────────┐
  │ A[0][0]   │ A[0][1]   │ A[0][2]   │ A[0][3]   │ A[0][4]   │
  └───────────┴───────────┴───────────┴───────────┴───────────┘
      ↑           ↑           ↑           ↑           ↑
     T0          T1          T2          T3          T4
  (스레드 인덱스) 
```
- 2차원에서, GPU 메모리는 32개 스레드(= 1워프)가 연속적인 메모리에 접근할때 가장 효율적.
- Numpy 배열은 행 우선(row-major) 방식이므로, 행 방향인 가로로 "연속적인 메모리 주소"를 갖는다.
- 한번의 트랜잭션으로 32개의 주소 요청을 처리할수 있게, 인접한 스레드는 인접한 열에 접근하도록 커널을 설계해야함.
- ex) 스레드 0, 1, 2가 각각 2차원 배열의 [row][0], [row][1], [row][2]에 접근 = 1번의 메모리 트랜잭션

  <img width="678" height="460" alt="image" src="https://github.com/user-attachments/assets/91a4cba3-a5d2-45bb-bc5d-e321d4ad9a3f" />

<img width="523" height="88" alt="image" src="https://github.com/user-attachments/assets/70838f67-aee5-4c65-ae3e-f4a9c911dfd6" /><br>
**-> Coalesced Kernel이 메모리상에 연속된 주소에 접근하는 병합 방식이므로, GPU 성능이 최적화되어 3배가량 빠르다.**

## 원자적 연산과 경쟁 상태
경쟁 상태(Race Condition) 문제
- 여러 스레드가 공유 자원에 동시에 접근할 때 발생
- `a += 1` 같은 비원자적 연산은 read → add → write로 분리된 세 단계로 실행되기때문에 공유 메모리 상황에선 경쟁 상태 문제가 생긴다.

  - ex) a = 0 이라는 초기상태에서 여러 스레드가 `a += 1` 연산을 동시에 실행한다면, <br>모든 스레드들은 자기가 바라보는 현재 상태인 a = 0 을 기준으로 +1 연산을 시도하여 +2, +3 이 되지 못하고, a = 1 로 계속 덮어씌워진다.

원자적 연산(Atomic Operation)
- 경쟁 상태 문제를 해결하고, 병렬 알고리즘의 정확성을 보장하는 방법
- `cuda.atomic.add(hist_out, pixel_value, 1)`

  <img width="588" height="464" alt="image" src="https://github.com/user-attachments/assets/6348fc18-7c49-4d99-978a-fe2debf8a300" /><br>
**-> hist_out[pixel_value] 위치에 대한 증가 연산을 하나씩 순차적으로 실행하여, 누락 없이 정확히 num_pixels만큼 더함**
