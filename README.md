# 35일차 - CUDA환경에서 GPU로 실시간 데이터 처리

## Numba란?
1. 정의
- 파이썬 함수를 고성능 기계어 코드로 변환하는 JIT(Just-In- Time) 컴파일러. 
- 등장배경 : 자율주행차는 초당 1920*1080 (FHD) 200만개의 픽셀 이미지 + 수백개의 거리, 속도 반환값 변환을 처리해야함
- 이 모든 데이터를 30FPS 기준 "33ms 이내"에 처리하여 인지, 판단, 제어를 완료해야함
- 직렬처리를 하는 CPU는 혼자서 200만개의 문제를 풀어야하고, 병렬처리를 하는 GPU는 수천명이서 픽셀 몇개씩을 나눠 맡아 동시에 처리하여 빠르다!

2. 작동원리
- 함수가 처음 호출될때, 입력되는 데이터 타입에 맞춰 "최적화된 코드"를 즉석에서 컴파일하는 것.
- CPU 메모리의 Numpy 배열을 GPU 메모리로 복사 -> 커널이 GPU에서 함수를 병렬로 실행시킴 (매우빠름) -> 다시 CPU 메모리로 결과 배열을 복사함.<br>(Host->Device->Host 순)
- 병목 : GPU의 연산이 아무리 빨라도 데이터 전송에 드는 시간이 존재하므로, 간단한 코드는 CPU에서만 하는게 좋다.
  
  <img width="565" height="682" alt="image" src="https://github.com/user-attachments/assets/886564f2-2a17-474e-a936-4d6cd3bf2ec5" /><br>
**-> 작은 데이터에선 GPU 가속을 해도 "데이터 전송시간" 비중이 더 커서 속도향상 체감이 제한적. (고작 0.4초 빠름)**<br>
**-> 큰 데이터에선 "연산시간"이 압도적으로 줄어들어, 동일한 "데이터 전송시간"을 가져도 걸린시간 총합이 적기에 빠르다고 느낀다. (0.7초 빠름!)**

3. 함정 : 치명적 상쇄(Catastrophic Cancellation)
- float32 = 약 7자리의 10진수 단정밀도
- float64 = 약 15~17자리의 배정밀도
- GPU는 보통 크기가 작은 float32 연산이 훨씬 빠르므로, 모든 연산이 float32로 일어나도록 데코레이터에 명시해준다.<br>
`@vectorize(['float32(float32)'], target='cuda')`<br>
- 정밀한 제어를 위해서라면 성능을 약간 희생하여 float64를 사용한다.<br>

  <img width="259" height="54" alt="image" src="https://github.com/user-attachments/assets/439c4d2d-afaa-48e0-a692-5492ad5a9341" /><br>
**-> float32연산은 1보다 매우작은 값을 무시하므로, 1e-8같은 작은 값을 반환하면 0이라고 나온다.**

## CUDA의 실행 모델 계층 구조 (Grid, Block, Thread)
GPU에 직접 명령을 내리는 커널
- Ufunc 자체는 간단하지만, 모든 연산이 배열의 각 원소에 독립적이여야만 사용가능.
- 이웃 픽셀 정보가 필요한 컨볼루션 등의 연산에는, 우리가 직접 스레드의 동작을 정의하는 "커스텀 CUDA 커널"을 작성해야함.
- nsys : 엔비디아의 프로파일러로, 코드의 어느 부분에서 시간이 소요되는지 정확히 알려줌.<br>
`nsys profile python my_script.py`
- 각 CUDA 커널의 평균 실행 시간, 메모리 복사에 소요된 시간, 메모리 처리량 [GB/s] 단위로 Coalescing이 잘 되었는지 판단하는 지표를 보여줌.

함정2. : 메모리 병합(Coalescing)
```
  ┌───────────┬───────────┬───────────┬───────────┬───────────┐
  │ A[0][0]   │ A[0][1]   │ A[0][2]   │ A[0][3]   │ A[0][4]   │
  └───────────┴───────────┴───────────┴───────────┴───────────┘
      ↑           ↑           ↑           ↑           ↑
     T0          T1          T2          T3          T4
  (스레드 인덱스) 
```
- GPU 메모리는 32개 스레드(= 1워프)가 연속적인 메모리에 접근할때 가장 효율적
- Numpy 배열은 행 우선(row-major) 방식이므로, 행 방향인 가로로 "연속적인 메모리 주소"를 갖는다.
- 한번의 트랜잭션으로 32개의 주소 요청을 처리할수 있게, 인접한 스레드는 인접한 열에 접근하도록 커널을 설계해야함.
- ex) 스레드 0, 1, 2가 각각 2차원 배열의 [row][0], [row][1], [row][2]에 접근 = 1번의 메모리 트랜잭션
```python
# A행렬의 열 순서를 가로축으로 보고, B행렬의 행을 채웠다.
# 출력되는 B행렬은 가로방향으로 진행하는 메모리 병합 접근 방식!
B[x, y] = A[y, x]
```

